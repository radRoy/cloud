author: Daniel Walther
creation date: 2023.05.11

CHECKPOINT_DIR = home/dwalth/data/wolny/checkpoints/checkpoints_230511-1patch
~/data/wolny/checkpoints/checkpoints_230511-1patch

CONFIG_FILE = home/dwalth/data/wolny/configs/config_230511-1patch/train_config_230511.yml
~/data/wolny/configs/config_230511-1patch/train_config_230511.yml

```bash
ssh into login1.cluster etc.
screen -S onepatch230511
srun --pty -n 1 -c 8 --gres=gpu:V100:1 --constraint=GPUMEM32GB --mem=128G --time=24:00:00 bash -l
	srun: job 1223786 queued and waiting for resources
```
```bash
squeue -s -u dwalth
```

continued on 2023.05.12:
```bash
#ssh into login1.cluster...
screen -ls
	There is a screen on:
    	    2065892.onepatch230511  (05/11/23 16:21:42)     (Detached)
	1 Socket in /run/screen/S-dwalth.
screen -x onepatch230511
	# inside screen
squeue -u dwalth
	JOBID    USER    STATE        CPU MIN_ME         TIME END_TIME             NODELIST(REASON)
	1223786  dwalth  RUNNING        8   128G     16:07:33 2023-05-12T16:35:26  u20-computeibmgpu-vesta8
nvidia-smi --list-gpus
	GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-7f2da8a0-8a7f-1f92-dac9-72cd0125ea67)
module load anaconda3
source activate envWolny
tensorboard --logdir ~/data/wolny/checkpoints/checkpoints_230511-1patch
	# trailing `/` absolutely required!
train3dunet --config ~/data/wolny/configs/config_230511-1patch/train_config_230511.yml
	# look at results at office
	# did not succeed.
	INFO Dataset - Slice builder config: ... # was the last info message before the first error
	ERROR HDF5Dataset - Skipping train set: ... # the info when encountering the 1st error
	File "/scratch/dwalth/pytorch-3dunet/pytorch3dunet/datasets/utils.py", line 121, in _gen_indices
		assert i >= k, 'Sample size has to be bigger than the patch size' # the traceback

# 2 debugging sessions below.

watch -n0.1 nvidia-smi
train3dunet --config ~/data/wolny/configs/config_230511-1patch/train_config_230511.yml
	# 17098MiB / 32768MiB

watch -n0.1 nvidia-smi

	Every 0.1s: nvidia-smi                                                                                                                                                                                                                  u20-computeibmgpu-vesta8: Fri May 12 11:50:05 2023

	Fri May 12 11:50:05 2023
	+-----------------------------------------------------------------------------+
	| NVIDIA-SMI 510.73.08    Driver Version: 510.73.08    CUDA Version: 11.6     |
	|-------------------------------+----------------------+----------------------+
	| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
	| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
	|                               |                      |               MIG M. |
	|===============================+======================+======================|
	|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
	| N/A   34C    P0    62W / 300W |  17098MiB / 32768MiB |      0%      Default |
	|                               |                      |                  N/A |
	+-------------------------------+----------------------+----------------------+

	+-----------------------------------------------------------------------------+
	| Processes:                                                                  |
	|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
	|        ID   ID                                                   Usage      |
	|=============================================================================|
	|    0   N/A  N/A   2801743      C   ...y7vcuhn5nw2s3i/bin/python    17095MiB |
	+-----------------------------------------------------------------------------+
```
- figure out what the max input size and patch & stride size is for a cluster node with 1xA100 card.
	- do this before running the train3dunet etc.
		- `watch -n0.1 nvidia-smi`
		- ctrl + Z
	- run the model

```bash
# ssh login1...
screen -S 1patch
srun --pty -n 1 -c 8 --gres=gpu:V100:1 --constraint=GPUMEM32GB --mem=32G --time=24:00:00 bash -l
	srun: job 1231877 queued and waiting for resources
squeue -s -u dwalth
			 STEPID     NAME PARTITION     USER      TIME NODELIST
		  1231877.0     bash  standard   dwalth      0:15 u20-computeibmgpu-vesta12
	 1231877.extern   extern  standard   dwalth      0:15 u20-computeibmgpu-vesta12
nvidia-smi --list-gpus
	GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-7d6cd181-d6ef-8af7-4e20-4de1a2f52b4c)
nvidia-smi -l 1
	# prints (updated) nvidia-smi output to stdout every 1 second

nvidia-smi -x -q
<?xml version="1.0" ?>
<!DOCTYPE nvidia_smi_log SYSTEM "nvsmi_device_v11.dtd">
<nvidia_smi_log>
        <timestamp>Fri May 12 13:24:22 2023</timestamp>
        <driver_version>510.73.08</driver_version>
        <cuda_version>11.6</cuda_version>
        <attached_gpus>1</attached_gpus>
        <gpu id="00000000:89:00.0">
                <product_name>Tesla V100-SXM2-32GB</product_name>
				...
				</clocks_throttle_reasons>
                <fb_memory_usage>
                        <total>32768 MiB</total>
                        <reserved>257 MiB</reserved>
                        <used>0 MiB</used>
                        <free>32510 MiB</free>
                </fb_memory_usage>
                <bar1_memory_usage>
                        <total>32768 MiB</total>
                        <used>4 MiB</used>
                        <free>32764 MiB</free>
                </bar1_memory_usage>
                <compute_mode>Default</compute_mode>
                <utilization>
                        <gpu_util>0 %</gpu_util>
                        <memory_util>0 %</memory_util>
                        <encoder_util>0 %</encoder_util>
                        <decoder_util>0 %</decoder_util>
                </utilization>
                <encoder_stats>
                        <session_count>0</session_count>
                        <average_fps>0</average_fps>
                        <average_latency>0</average_latency>
                </encoder_stats>
                <fbc_stats>
                        <session_count>0</session_count>
                        <average_fps>0</average_fps>
                        <average_latency>0</average_latency>
                </fbc_stats>
                <ecc_mode>
				...
			...
		...
	...
</nvidia_smi_log>

nvidia-smi -f <output file>  # redirect output to a file instead of stdout. existing file will be overwritten
nvidia-smi -d TYPE, --display=TYPE
	Display only selected information: MEMORY, UTILIZATION, ECC, TEMPERA-
	TURE, POWER, CLOCK, COMPUTE, PIDS, PERFORMANCE, SUPPORTED_CLOCKS,
	PAGE_RETIREMENT, ACCOUNTING Flags can be combined with comma e.g.
	"MEMORY,ECC". Sampling data with max, min and avg is also returned for
	POWER, UTILIZATION and CLOCK display types. Doesn't work with
	-u/--unit or -x/--xml-format flags.
# ... could explore this endlessly... losing concentration.

# running with ~1 patch (or huge patch size, rather...)
screen -ls
	There is a screen on:
			2773042.1patch  (05/12/23 13:09:45)     (Detached)
	1 Socket in /run/screen/S-dwalth.
screen -x 1patch
load anaconda3
source activate envWolny
tensorboard --logdir ~/data/wolny/checkpoints/checkpoints_230511-1patch/
train3dunet --config ~/data/wolny/configs/config_230511-1patch/train_config_230511.yml
# Detached
dwalth@u20-login-1:~$ squeue -u dwalth
JOBID    USER    STATE        CPU MIN_ME         TIME END_TIME             NODELIST(REASON)
1231877  dwalth  RUNNING        8    32G        31:16 2023-05-13T13:19:45  u20-computeibmgpu-vesta12
dwalth@u20-login-1:~$ screen -ls
There is a screen on:
        2773042.1patch  (05/12/23 13:09:45)     (Detached)
# logout and let it run.
```

## Debugging of the `assert i >= k, 'Sample size has to be bigger than the patch size'` error message

The cause of the error was the specification of the patch shape. The patch shape in the `train_config.yml` takes as input [z, y, x] instead of the previously given [z, x, y]. This has been corrected.

## Debugging of the `...empty iterable...` error message (which was more cryptic)

This message occurred, when, I hypothesize, the patch shape was too large to leave room for the stride shape to convolve over a patch:
- In this case, the patch shape was 1 pixel less (in each dimension [z,y,x]) than the image resolution, and the stride shape was 1 pixel in every dimension [1,1,1].
- I tried out a patch shape of 2 pixels less in each dimension [z,y,x], so that the unchanged stride shape of [1,1,1] can, hypothetically, make the kernel (of the same size as the patch) convolve over the input patch in both directions in each dimension (so with stride of [1,1,1], stride/convolve over the patch 1 pixel back & 1 pixel forth; this in each dimension).

## Debugging the tensorboard science app not showing any diagnostics:

I simply forgot the trailing `/` when specifying the `tensorboard --logdir`

## By the way, some other interesting information on the slice building part in the 3dunet:

In the (same as above) file [`pytorch-3dunet/pytorch3dunet/datasets/utils.py`](https://github.com/wolny/pytorch-3dunet/blob/master/pytorch3dunet/datasets/utils.py), in the `class SliceBuilder`:
```python
def __init__(self, raw_dataset, label_dataset, weight_dataset, patch_shape, stride_shape, **kwargs):
	patch_shape = tuple(patch_shape)
    stride_shape = tuple(stride_shape)
    skip_shape_check = kwargs.get('skip_shape_check', False)
    if not skip_shape_check:
		self._check_patch_shape(patch_shape)  # this is the important line.
```
The marked line calls this method (in the same class) - some **important patch shape reqirements**:
```python
@staticmethod
    def _check_patch_shape(patch_shape):
        assert len(patch_shape) == 3, 'patch_shape must be a 3D tuple'
        assert patch_shape[1] >= 64 and patch_shape[2] >= 64, 'Height and Width must be greater or equal 64'  # this is the informative line
```

## Protocol of Saturday, 2023.05.13
```bash
# in login1 node
screen -ls
	There is a screen on:
        	2773042.1patch  (05/12/23 13:09:44)     (Attached)
srun --pty -n 1 -c 8 --gres=gpu:A100 --mem=128G --time=24:00:00 bash -l
	job 1241836 queued and waiting for resources
	job 1241836 has been allocated resources
squeue -s -u dwalth
			STEPID     NAME PARTITION     USER      TIME NODELIST
	 1241764.batch    batch  standard   dwalth      5:18 u20-compute-m2
	1241764.extern   extern  standard   dwalth      5:18 u20-compute-m2
	     1241836.0     bash  standard   dwalth      1:29 u20-computeibmgpu-vesta20
	1241836.extern   extern  standard   dwalth      1:29 u20-computeibmgpu-vesta20
squeue -u dwalth
	JOBID    USER    STATE        CPU MIN_ME         TIME END_TIME             NODELIST(REASON)
	1241836  dwalth  RUNNING        8   128G         2:26 2023-05-14T18:30:53  u20-computeibmgpu-vesta20
	1241764  dwalth  RUNNING        2  7800M         6:15 2023-05-13T19:27:04  u20-compute-m2

# pwd is ~ (home/dwalth/)
tensorboard --logdir data/wolny/checkpoints/checkpoints_230511-1patch/
	# verbose...
	
	2023-05-13 18:39:08.140118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
	To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
	2023-05-13 18:39:08.736379: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:08.736412: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
	2023-05-13 18:39:11.309486: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:11.309715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:11.309725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
	2023-05-13 18:39:16.701306: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:16.701691: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:16.701888: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:16.702082: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:16.702268: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:16.702449: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:16.702631: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:16.702818: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/munge-0.5.14/lib:/cluster/slurm-20-11-8-1/lib:/cluster/pmix-4.1.2/lib:/cluster/libevent-2.1.12/lib
	2023-05-13 18:39:16.702846: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
	Skipping registering GPU devices...

	NOTE: Using experimental fast data loading logic. To disable, pass
		"--load_fast=false" and report issues on GitHub. More details:
		https://github.com/tensorflow/tensorboard/issues/4784

	Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
	TensorBoard 2.11.2 at http://localhost:6006/ (Press CTRL+C to quit)

train3dunet --config ~/data/wolny/configs/config_230511-1patch/train_config_230511.yml

# DW: this tensorboard logging did not work... nothing appears in the tensorboard browser app... why?
```

## Protocol of Monday, 15.05.2023
```bash
# ssh login1.cluster ...
screen -ls
	No Sockets found in /run/screen/S-dwalth.
squeue -u dwalth
	JOBID    USER    STATE        CPU MIN_ME         TIME END_TIME             NODELIST(REASON)
	# empty
screen -S tblogdirtest1
	# tensorboard logdir test 1 (with trailing `/` in the train config file.)
srun --pty -n 1 -c 8 --gres=gpu --mem=128G --time=24:00:00 bash -l
	srun: job 1263008 queued and waiting for resources
	srun: job 1263008 has been allocated resources
squeue -u dwalth
	JOBID    USER    STATE        CPU MIN_ME         TIME END_TIME             NODELIST(REASON)
	1263008  dwalth  RUNNING        8   128G         0:26 2023-05-16T09:37:07  u20-computeibmgpu-vesta8
nvidia-smi --list-gpus
	GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-3bc869c3-2f9e-18a7-d2fa-d4f9d8cec5d8)
module load anaconda3; source activate envWolny  # works
tensorboard --logdir home/dwalth/data/wolny/checkpoints/checkpoints_230511-1patch/
	# copy the DIR from the config file 1to1
# transfer the adapted train config file
train3dunet --config ~/data/wolny/configs/config_230511-1patch/train_config_230511.yml
```